{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mah20012/envs/cse1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "base_to_int = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'PAD': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821840\n",
      "821840\n"
     ]
    }
   ],
   "source": [
    "# takes some time\n",
    "df = pd.read_csv('../train_data.csv')\n",
    "\n",
    "df_2A3 = df[df['experiment_type'] == '2A3_MaP']\n",
    "df_DMS = df[df['experiment_type'] == 'DMS_MaP']\n",
    "print(len(df_2A3))\n",
    "print(len(df_DMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "# df_2A3 = df_2A3.sample(n=int(len(df_2A3)/4), replace=False)\n",
    "df_2A3 = df_2A3.sample(n=15000, replace=False)\n",
    "df_DMS = df_DMS.sample(n=15000, replace=False)\n",
    "print(len(df_2A3))\n",
    "print(len(df_DMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, dataframe, max_length, context_size=3):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_length = max_length\n",
    "        self.context_size = context_size\n",
    "        self.pad_id = base_to_int['PAD']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        padded_sequence = self.encode_seq(sequence)\n",
    "        \n",
    "        # reactivity_0001 -> reactivity_0206\n",
    "        reactivity = self.dataframe.iloc[idx, 6:212].fillna(0).values.astype(float)\n",
    "        reactivity_padded = list(reactivity) + [0]*(self.max_length - len(reactivity))\n",
    "        reactivity_padded = reactivity_padded[:self.max_length]\n",
    "        \n",
    "        mask = [1 if i < len(padded_sequence) else 0 for i in range(self.max_length)]\n",
    "        \n",
    "        return padded_sequence, torch.tensor(reactivity_padded, dtype=torch.float), torch.tensor(mask, dtype=torch.float)\n",
    "        \n",
    "    \n",
    "    def encode_seq(self, sequence):\n",
    "        # Convert sequence to list of integers using the dictionary\n",
    "        encoded_sequence = [base_to_int[base] for base in sequence]\n",
    "        \n",
    "        # Pad the sequence to the specified max_length\n",
    "        padded_sequence = encoded_sequence + [base_to_int['PAD']] * (self.max_length - len(encoded_sequence))\n",
    "        padded_sequence = padded_sequence[:self.max_length]\n",
    "\n",
    "        return torch.tensor(padded_sequence, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(df_2A3['sequence'].apply(len))\n",
    "dataset = RNADataset(df_2A3, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 0, 0, 1, 2, 0, 1, 3, 1, 2, 0, 2, 3, 0, 2, 0, 2, 3, 1, 2, 0, 0,\n",
       "         0, 0, 3, 1, 3, 3, 0, 3, 3, 0, 3, 3, 1, 0, 3, 3, 2, 3, 3, 0, 0, 3, 2, 1,\n",
       "         1, 3, 0, 3, 1, 3, 3, 0, 0, 1, 1, 3, 3, 2, 0, 1, 1, 0, 2, 2, 2, 1, 3, 3,\n",
       "         3, 0, 0, 1, 3, 2, 1, 0, 2, 0, 2, 3, 1, 0, 1, 0, 3, 2, 3, 3, 2, 0, 1, 0,\n",
       "         1, 3, 2, 0, 1, 3, 3, 0, 0, 1, 0, 0, 0, 2, 1, 1, 3, 3, 0, 1, 0, 3, 3, 0,\n",
       "         0, 2, 3, 2, 2, 2, 2, 0, 1, 0, 1, 3, 2, 3, 3, 1, 1, 0, 1, 3, 3, 1, 2, 2,\n",
       "         3, 2, 2, 0, 0, 1, 0, 2, 3, 2, 3, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]),\n",
       " tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.3590,  2.3790,\n",
       "          0.6800,  2.7180,  0.6800,  0.4410,  1.3240,  0.6800,  1.0190,  1.0190,\n",
       "          2.0390,  0.3400, -0.5460,  1.6990,  0.5910,  0.4950,  0.6800,  1.3590,\n",
       "          0.6800,  0.6800,  0.6800,  0.0000,  2.7180, -1.8390, -0.2070,  0.0000,\n",
       "         -1.2260,  0.6800,  0.3400,  0.0000,  1.0190,  1.0190,  0.3400,  1.6990,\n",
       "          0.1700,  0.1700,  0.0000,  0.3400, -0.6130, -1.8390, -0.6130,  0.0000,\n",
       "         -1.8390,  1.0190,  1.2460,  1.1330,  0.6800,  0.6800, -1.8390,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.2730,  0.0000,  0.0000,  0.0000,  0.6800,\n",
       "          1.0190,  1.3590,  0.3400,  1.0190,  0.0000, -0.2730,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  1.6990,  4.4170,  0.3400,  0.3400,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6130,  0.0000, -0.6130,\n",
       "          0.3400,  0.1700,  0.1700,  0.0000,  0.0000,  0.6800,  0.3400,  1.0190,\n",
       "          2.3790,  0.0000,  0.0000,  0.0000,  0.0000,  1.0190,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([206]), torch.Size([206]), torch.Size([206]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequence, reactivity_padded, mask_padded = dataset[0]\n",
    "padded_sequence.shape, reactivity_padded.shape, mask_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.6, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(MaskedHuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        clipped_target = torch.clip(target, min=0, max=1)\n",
    "\n",
    "        loss = torch.where(torch.abs(input - clipped_target) < self.delta,\n",
    "                           0.5 * (input - clipped_target) ** 2,\n",
    "                           self.delta * (torch.abs(input - clipped_target) - 0.5 * self.delta))\n",
    "        loss = loss * mask\n",
    "        return loss.sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(base_to_int)  # Vocabulary size\n",
    "emsize = 300  # Increase embedding dimensions to 300\n",
    "nhid = 300    # Increase hidden layer size to 300\n",
    "nlayers = 2   # Increase the number of layers in the transformer to 6\n",
    "nhead = 100   # Increase the number of heads in multi-head attention to 6\n",
    "dropout = 0.6 # Increase dropout value to 0.6\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Data loading and preprocessing\n",
    "max_length = max(df_2A3['sequence'].apply(len))\n",
    "dataset = RNADataset(df_2A3, max_length)\n",
    "\n",
    "# Setting up cross-validation\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 206]) torch.Size([128, 206]) torch.Size([128, 206])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "for sequences, reactivities, masks in dataloader:\n",
    "    print(sequences.shape, reactivities.shape, masks.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: isinstance() arg 2 must be a type, a tuple of types, or a union\n\nCUDA call was originally invoked at:\n\n['  File \"/home/mah20012/envs/cse1/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/runpy.py\", line 86, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\\n    app.start()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\\n    self._run_once()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\\n    handle._run()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/events.py\", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_51935/1576177217.py\", line 2, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/__init__.py\", line 778, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 170, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 168, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py:230\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py:116\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 116\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py:345\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py:360\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    359\u001b[0m _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/_utils.py:32\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m device\u001b[38;5;241m.\u001b[39midx\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type, a tuple of types, or a union",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the model and loss function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m MaskedHuberLoss()\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model1\u001b[38;5;241m.\u001b[39mparameters(), weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-3\u001b[39m)\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/nn/modules/module.py:625\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py:234\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00morig_traceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: isinstance() arg 2 must be a type, a tuple of types, or a union\n\nCUDA call was originally invoked at:\n\n['  File \"/home/mah20012/envs/cse1/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/runpy.py\", line 86, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\\n    app.start()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\\n    self._run_once()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\\n    handle._run()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/asyncio/events.py\", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_51935/1576177217.py\", line 2, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/__init__.py\", line 778, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 170, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/mah20012/envs/cse1/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 168, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_index)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=160, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=160, shuffle=False)\n",
    "\n",
    "    # Initialize the model and loss function\n",
    "    model1 = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    loss_fn = MaskedHuberLoss()\n",
    "    optimizer = optim.Adam(model1.parameters(), weight_decay=0.0001, lr=5e-3)\n",
    "    \n",
    "    # Setting up the scheduler\n",
    "    def lambda_epoch(epoch):\n",
    "        max_epoch = 3\n",
    "        return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model1.train()\n",
    "        total_loss = 0.0\n",
    "        for batch, (seq, target, mask) in enumerate(train_dataloader):\n",
    "            seq = seq.to(device)\n",
    "            target = target.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model1(seq)\n",
    "            loss = loss_fn(output, target, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % 300 == 0 and batch > 0:\n",
    "                current = batch * len(seq)\n",
    "                total = len(train_dataloader.dataset)\n",
    "                elapsed = time.time() \n",
    "                loss_per_batch = total_loss / batch\n",
    "                print(f'| Epoch {epoch+1:3d}/{num_epochs:3d} | {current:5d}/{total:5d} sequences | Mean Loss Batch: {loss_per_batch} | Elapsed time {elapsed - start_time:.2f}s')\n",
    "        \n",
    "        # Execute the learning rate scheduler once\n",
    "        scheduler.step()\n",
    "    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1:3d} completed in {elapsed_time:.2f}s, Total loss {total_loss:.4f}')\n",
    "            \n",
    "    torch.save(model1.state_dict(), 'model1.pth')\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model1.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, target, mask) in enumerate(test_dataloader):  # Unpacking four elements\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target, mask)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "        print(f\"Fold {fold + 1} Test Loss: {total_test_loss / len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ntokens = len(base_to_int)  # Vocabulary size\n",
    "emsize = 300  # Increase embedding dimensions to 300\n",
    "nhid = 300   # Increase hidden layer size to 300\n",
    "nlayers = 2  # Increase the number of layers in the transformer to 6\n",
    "nhead = 100  # Increase the number of heads in multi-head attention to 6\n",
    "dropout = 0.6  # Increase dropout value to 0.6\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Data loading and preprocessing\n",
    "max_length = max(df_DMS['sequence'].apply(len))\n",
    "dataset = RNADataset(df_DMS, max_length)\n",
    "\n",
    "# Setting up cross-validation\n",
    "kf = KFold(n_splits=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_index)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=160, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=160, shuffle=False)\n",
    "\n",
    "    # Initialize model and loss function\n",
    "    model2 = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    loss_fn = MaskedHuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=0.0001, lr=5e-3)\n",
    "    \n",
    "    # Setting up the scheduler\n",
    "    def lambda_epoch(epoch):\n",
    "        max_epoch = 3\n",
    "        return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model2.train()\n",
    "        total_loss = 0.0\n",
    "        for batch, (seq, target, mask) in enumerate(train_dataloader):\n",
    "            seq = seq.to(device)\n",
    "            target = target.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model2(seq)\n",
    "            loss = loss_fn(output, target, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "\n",
    "            if batch % 300 == 0 and batch > 0:\n",
    "                current = batch * len(seq)\n",
    "                total = len(train_dataloader.dataset)\n",
    "                elapsed = time.time() \n",
    "                loss_per_batch = total_loss / batch\n",
    "                print(f'| Epoch {epoch+1:3d}/{num_epochs:3d} | {current:5d}/{total:5d} sequences | Mean Loss Batch: {loss_per_batch} | Elapsed time {elapsed - start_time:.2f}s')\n",
    "        \n",
    "        # Execute learning rate scheduler once\n",
    "        scheduler.step()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1:3d} completed in {elapsed_time:.2f}s, Total loss {total_loss:.4f}')\n",
    "            \n",
    "    torch.save(model2.state_dict(), 'model2.pth')\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, target, mask) in enumerate(test_dataloader):  # Unpacking four elements\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target, mask)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "        print(f\"Fold {fold + 1} Test Loss: {total_test_loss / len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "df_test = pd.read_csv('../test_sequences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset_Inference(Dataset):\n",
    "    def __init__(self, dataframe, max_length, context_size=3):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_length = max_length\n",
    "        self.context_size = context_size\n",
    "        self.pad_id = base_to_int['PAD']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx, 3]\n",
    "\n",
    "        padded_sequence = self.encode_seq(sequence)\n",
    "\n",
    "        return padded_sequence\n",
    "    \n",
    "    def encode_seq(self, sequence):\n",
    "        # Convert sequence to list of integers using the dictionary\n",
    "        encoded_sequence = [base_to_int[base] for base in sequence]\n",
    "        \n",
    "        # Pad the sequence to the specified max_length\n",
    "        padded_sequence = encoded_sequence + [base_to_int['PAD']] * (self.max_length - len(encoded_sequence))\n",
    "        padded_sequence = padded_sequence[:self.max_length]\n",
    "\n",
    "        return torch.tensor(padded_sequence, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('model1_updated.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(df_test['sequence'].apply(len))\n",
    "dataset_infer = RNADataset_Inference(df_test, max_length)\n",
    "infer_dataloader = DataLoader(dataset_infer, batch_size=50, shuffle=False)\n",
    "\n",
    "ntokens = len(base_to_int)  # Size of the vocabulary\n",
    "emsize = 300  # Dimension of embeddings\n",
    "nhid = 300    # Size of the hidden layer\n",
    "nlayers = 2   # Number of layers in the transformer\n",
    "nhead = 100   # Number of heads in multi-head attention\n",
    "dropout = 0.6 # Dropout rate\n",
    "\n",
    "# Initialize the model\n",
    "# model_2A3 = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "# model_2A3.load_state_dict(torch.load('model1.pth'))\n",
    "model_2A3 = copy.deepcopy(model1)\n",
    "model_2A3.eval().to(device)\n",
    "\n",
    "# model_DMS = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "# model_DMS.load_state_dict(torch.load('model1_updated.pth'))\n",
    "model_DMS = copy.deepcopy(model2)\n",
    "model_DMS.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_infer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(infer_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out_2A3 = model_2A3(seq)#.detach().cpu().numpy()\n",
    "# model = model_2A3.to(device)\n",
    "# model(seq[:50])\n",
    "del model, model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# torch.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ids = np.empty(shape = (0,1),dtype=int)\n",
    "# preds_2A3 = np.empty(shape = (0,1),dtype=np.float32)\n",
    "# preds_DMS = np.empty(shape = (0,1),dtype=np.float32)\n",
    "\n",
    "# with torch.no_grad():\n",
    "    # for seq in tqdm(infer_dataloader):\n",
    "    # #     print(seq)\n",
    "    #     seq = seq.to(device)\n",
    "    #     out_2A3 = model_2A3(seq[:50]).detach().cpu().numpy()\n",
    "    #     out_DMS = model_DMS(seq[:50]).detach().cpu().numpy()\n",
    "        \n",
    "    #     clipped_out_2A3 = np.clip(out_2A3, a_min=0,  a_max=1)\n",
    "    #     clipped_out_DMS = np.clip(out_DMS, a_min=0, a_max=1)\n",
    "    \n",
    "    \n",
    "    #     preds_2A3 = np.append(preds_2A3, clipped_out_2A3)\n",
    "    #     preds_DMS = np.append(preds_DMS, clipped_out_DMS)\n",
    "    #     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df = pd.DataFrame({'id':np.arange(0, len(preds_2A3), 1),\"reactivity_2A3_MaP\": preds_2A3,\"reactivity_DMS_MaP\": preds_DMS})\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# submission_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# predictions = []\n",
    "\n",
    "# for sequence in tqdm(df_test['sequence'], desc=f'Processing split {i}'):\n",
    "#     # Preprocess sequence\n",
    "#     context_padded_sequence = get_context_padded_sequence(sequence)\n",
    "#     padded_sequence = context_padded_sequence + [base_to_int['PAD']] * (max_length - len(context_padded_sequence))\n",
    "#     sequence_tensor = torch.tensor(padded_sequence, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "#     # Prediction\n",
    "#     with torch.no_grad():\n",
    "#         prediction = model(sequence_tensor)\n",
    "\n",
    "#     # Get and process prediction results\n",
    "#     predicted_reactivity = prediction.squeeze(0).tolist()\n",
    "#     predicted_reactivity_trimmed = predicted_reactivity[:len(sequence)]\n",
    "#     predictions.append(predicted_reactivity_trimmed)\n",
    "    \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     estimated_remaining_time = (elapsed_time / (i + 1)) * (num_splits - i - 1)\n",
    "#     print(f\"Completed split {i}. Estimated remaining time: {estimated_remaining_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    \n",
    "# submission = pd.DataFrame({'id':np.arange(0, len(concat_preds), 1), 'reactivity_DMS_MaP':concat_preds[:,1], 'reactivity_2A3_MaP':concat_preds[:,0]})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6923401,
     "sourceId": 51294,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
